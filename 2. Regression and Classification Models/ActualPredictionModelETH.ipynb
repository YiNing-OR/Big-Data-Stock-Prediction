{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from LinearRegression import run_linear_regression\n",
    "from RandomForestRegressor_New import RandomForestRegressor\n",
    "from RandomForestRegressor import RandomForestRegressor_Base\n",
    "from XGBoostRegressor import XGBoostRegressor\n",
    "from DecisionTreeRegressor import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from RidgeLinearRegression import run_ridge_regression\n",
    "from LassoLinearRegression import run_lasso_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('/Users/oryining/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/1. Data Retrieval and Processing of News and Stock Prices/ETHUSDT_Sentiments_PriceChange.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split and creating of x_train, x_val, x and y_train, y_val, y_test\n",
    "Y = merged_df[['PriceChange']]  \n",
    "X = merged_df[['weighted_positive_fb', 'weighted_negative_fb', 'weighted_neutral_fb','weighted_DocTone']]  \n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.2, random_state=1616)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=1616)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Models to input to Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearRegressionScratch():\n",
    "    #Linear Regresion Model\n",
    "    lin_reg_model = run_linear_regression(x_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    lin_reg_predicted_values = lin_reg_model.predict(x_test.values)\n",
    "\n",
    "    train_score = lin_reg_model.score(x_train.values, y_train.values.ravel())\n",
    "    test_score = lin_reg_model.score(x_test.values, y_test.values.ravel())\n",
    "    train_rmse = lin_reg_model.rmse(x_train.values, y_train.values.ravel())\n",
    "    test_rmse = lin_reg_model.rmse(x_test.values, y_test.values.ravel())\n",
    "\n",
    "    print(f\"Training RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "    print(f\"Training R2: {train_score}\")\n",
    "    print(f\"Test R2: {test_score}\")\n",
    "\n",
    "\n",
    "    return lin_reg_predicted_values,lin_reg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RidgeRegressionScratch():\n",
    "    #Ridge Regresion Model\n",
    "    rig_reg_model = run_ridge_regression(x_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    rig_reg_predicted_values = rig_reg_model.predict(x_test.values)\n",
    "\n",
    "    train_score = rig_reg_model.score(x_train.values, y_train.values.ravel())\n",
    "    test_score = rig_reg_model.score(x_test.values, y_test.values.ravel())\n",
    "    train_rmse = rig_reg_model.rmse(x_train.values, y_train.values.ravel())\n",
    "    test_rmse = rig_reg_model.rmse(x_test.values, y_test.values.ravel())\n",
    "\n",
    "    print(f\"Training RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "    print(f\"Training R2: {train_score}\")\n",
    "    print(f\"Test R2: {test_score}\")\n",
    "\n",
    "\n",
    "    return rig_reg_predicted_values,rig_reg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor \n",
    "def RandomForestRegressorScratch_New():\n",
    "\n",
    "    rfr_model = RandomForestRegressor(n_estimators=5, max_depth=5, min_samples_split=8)\n",
    "    rfr_model.fit(x_train.values, y_train.values.ravel())\n",
    "    rfr_predicted_values = rfr_model.predict(x_test.values)\n",
    "\n",
    "    train_score = rfr_model.score(x_train.values, y_train.values.ravel())\n",
    "    test_score = rfr_model.score(x_test.values, y_test.values.ravel())\n",
    "    train_rmse = rfr_model.rmse(x_train.values, y_train.values.ravel())\n",
    "    test_rmse = rfr_model.rmse(x_test.values, y_test.values.ravel())\n",
    "\n",
    "    print(f\"Training RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "\n",
    "    print(f\"Training R2: {train_score}\")\n",
    "    print(f\"Test R2: {test_score}\")\n",
    "\n",
    "    return rfr_predicted_values,rfr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor \n",
    "def RandomForestRegressorScratch_Base():\n",
    "\n",
    "    rfr_model = RandomForestRegressor_Base(n_estimators=5, max_depth=5, min_samples_split=8)\n",
    "    rfr_model.fit(x_train.values, y_train.values.ravel())\n",
    "    rfr_predicted_values = rfr_model.predict(x_test.values)\n",
    "\n",
    "    train_score = rfr_model.score(x_train.values, y_train.values.ravel())\n",
    "    test_score = rfr_model.score(x_test.values, y_test.values.ravel())\n",
    "    train_rmse = rfr_model.rmse(x_train.values, y_train.values.ravel())\n",
    "    test_rmse = rfr_model.rmse(x_test.values, y_test.values.ravel())\n",
    "\n",
    "    print(f\"Training RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "\n",
    "    print(f\"Training R2: {train_score}\")\n",
    "    print(f\"Test R2: {test_score}\")\n",
    "\n",
    "    return rfr_predicted_values,rfr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def evaluate_metrics(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"Mean Absolute Error (MAE)\": mae,\n",
    "        \"Mean Squared Error (MSE)\": mse,\n",
    "        \"Root Mean Squared Error (RMSE)\": rmse,\n",
    "        \"R-Squared (R2)\": r2\n",
    "    }\n",
    "\n",
    "# Main XGBoost Regressor function with feature engineering and extended evaluation\n",
    "def XGBoostRegressorScratch():\n",
    "    # Apply feature engineering on training and test data\n",
    "\n",
    "    # Initialize the model with regularization parameters\n",
    "    xgb_model = XGBoostRegressor(n_estimators=100, learning_rate=0.2, max_depth=8)\n",
    "    xgb_model.fit(x_train.values, y_train.values.ravel())\n",
    "\n",
    "    # Predictions on the test set\n",
    "    xgb_predicted_values = xgb_model.predict(x_test.values)\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_metrics = evaluate_metrics(xgb_model, x_train.values, y_train.values.ravel())\n",
    "    test_metrics = evaluate_metrics(xgb_model, x_test.values, y_test.values.ravel())\n",
    "\n",
    "    # Print training metrics\n",
    "    print(\"Training Metrics:\")\n",
    "    for metric, value in train_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    # Print test metrics\n",
    "    print(\"\\nTest Metrics:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "    return xgb_predicted_values, xgb_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Regressor\n",
    "def DecisionTreeScratch_New():\n",
    "\n",
    "    dtr_model = DecisionTreeRegressor(max_depth=5,min_samples_split=2, min_samples_leaf=1, max_features=1, min_impurity_decrease=0.0)\n",
    "    dtr_model.fit(x_train.values, y_train.values.ravel())\n",
    "    dtr_predicted_values = dtr_model.predict(x_test.values)\n",
    "\n",
    "    train_score = dtr_model.score(x_train.values, y_train.values.ravel())\n",
    "    test_score = dtr_model.score(x_test.values, y_test.values.ravel())\n",
    "    train_rmse = dtr_model.rmse(x_train.values, y_train.values.ravel())\n",
    "    test_rmse = dtr_model.rmse(x_test.values, y_test.values.ravel())\n",
    "\n",
    "    print(f\"Training RMSE: {train_rmse}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "\n",
    "    print(f\"Training R2: {train_score}\")\n",
    "    print(f\"Test R2: {test_score}\")\n",
    "\n",
    "    return dtr_predicted_values,dtr_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built Linear Regresion Outputs=====================\n",
      "Training RMSE: 2.713068820561519\n",
      "Test RMSE: 2.6959288615739627\n",
      "Training R2: 0.5768877671623378\n",
      "Test R2: 0.5872218339309225\n"
     ]
    }
   ],
   "source": [
    "print(\"===============Self-Built Linear Regresion Outputs=====================\")\n",
    "lin_reg_predicted_values, lin_reg_model= LinearRegressionScratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built Linear Regresion Outputs=====================\n",
      "Training RMSE: 2.713076459421345\n",
      "Test RMSE: 2.6959434481694924\n",
      "Training R2: 0.5768853845470774\n",
      "Test R2: 0.5872173671628809\n"
     ]
    }
   ],
   "source": [
    "print(\"===============Self-Built Linear Regresion Outputs=====================\")\n",
    "rig_reg_predicted_values,rig_reg_model = RidgeRegressionScratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Decision Tree Regresion Outputs=====================\n",
      "Training RMSE: 2.416596477918022\n",
      "Test RMSE: 2.4196898715828588\n",
      "Training R2: 0.6643070370681423\n",
      "Test R2: 0.6674788497527593\n"
     ]
    }
   ],
   "source": [
    "print(\"===============Decision Tree Regresion Outputs=====================\")\n",
    "dtr_predicted_values,dtr_model= DecisionTreeScratch_New()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built RandomForest Regressor Outputs================\n",
      "Training RMSE: 2.386097135710463\n",
      "Test RMSE: 2.3896219935112692\n",
      "Training R2: 0.6727269832500633\n",
      "Test R2: 0.6756915426200156\n"
     ]
    }
   ],
   "source": [
    "print(\"===============Self-Built RandomForest Regressor Outputs================\")\n",
    "rfr_predicted_values,rfr_model = RandomForestRegressorScratch_New()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built RandomForest Regressor Outputs================\n",
      "Training RMSE: 2.3849288563777096\n",
      "Test RMSE: 2.386038774906074\n",
      "Training R2: 0.6730473831984493\n",
      "Test R2: 0.6766634091710115\n"
     ]
    }
   ],
   "source": [
    "print(\"===============Self-Built RandomForest Regressor Outputs================\")\n",
    "rfr_predicted_values_base,rfr_model_base = RandomForestRegressorScratch_Base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built XGBoost Regressor Baseline Outputs============\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===============Self-Built XGBoost Regressor Baseline Outputs============\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m xgb_predicted_values,xgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mXGBoostRegressorScratch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mXGBoostRegressorScratch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mXGBoostRegressorScratch\u001b[39m():\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Apply feature engineering on training and test data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Initialize the model with regularization parameters\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     xgb_model \u001b[38;5;241m=\u001b[39m XGBoostRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Predictions on the test set\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     xgb_predicted_values \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(x_test\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/2. Regression and Classification Models/XGBoostRegressor.py:21\u001b[0m, in \u001b[0;36mXGBoostRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Fit a decision tree to the residuals\u001b[39;00m\n\u001b[1;32m     20\u001b[0m tree \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Update predictions\u001b[39;00m\n\u001b[1;32m     24\u001b[0m update \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[0;32m~/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/2. Regression and Classification Models/DecisionTreeRegressor.py:14\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/2. Regression and Classification Models/DecisionTreeRegressor.py:31\u001b[0m, in \u001b[0;36mDecisionTreeRegressor._build_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(left_indices) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(right_indices) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m     30\u001b[0m             left_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X[left_indices], y[left_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m             right_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (best_feature, best_threshold, left_tree, right_tree)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Else, Return the mean value if stopping criteria is met\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/2. Regression and Classification Models/DecisionTreeRegressor.py:31\u001b[0m, in \u001b[0;36mDecisionTreeRegressor._build_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(left_indices) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(right_indices) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m     30\u001b[0m             left_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X[left_indices], y[left_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m             right_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (best_feature, best_threshold, left_tree, right_tree)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Else, Return the mean value if stopping criteria is met\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: DecisionTreeRegressor._build_tree at line 31 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/2. Regression and Classification Models/DecisionTreeRegressor.py:31\u001b[0m, in \u001b[0;36mDecisionTreeRegressor._build_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(left_indices) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(right_indices) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_leaf:\n\u001b[1;32m     30\u001b[0m             left_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X[left_indices], y[left_indices], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m             right_tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (best_feature, best_threshold, left_tree, right_tree)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Else, Return the mean value if stopping criteria is met\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/2. Regression and Classification Models/DecisionTreeRegressor.py:19\u001b[0m, in \u001b[0;36mDecisionTreeRegressor._build_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     17\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth):\n\u001b[0;32m---> 19\u001b[0m     best_feature, best_threshold, best_mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_feature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mvar(y) \u001b[38;5;241m-\u001b[39m best_mse) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease: \n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# Special Feature : Early Stopping Based on Impurity Decrease (min_impurity_decrease):\u001b[39;00m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;66;03m# Only splits if the decrease in variance (impurity) is above a certain threshold. \u001b[39;00m\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;66;03m# Checked in _build_tree before continuing with a split.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         left_indices \u001b[38;5;241m=\u001b[39m X[:, best_feature] \u001b[38;5;241m<\u001b[39m best_threshold\n",
      "File \u001b[0;32m~/Documents/NUS MComp/Semester 2/CS5344 - Big Data Analytics Technology/CS5344_Project/2. Regression and Classification Models/DecisionTreeRegressor.py:52\u001b[0m, in \u001b[0;36mDecisionTreeRegressor._best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     49\u001b[0m right_indices \u001b[38;5;241m=\u001b[39m X[:, feature] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(left_indices) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(right_indices):\n\u001b[0;32m---> 52\u001b[0m     left_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((y[left_indices] \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     53\u001b[0m     right_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((y[right_indices] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y[right_indices])) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     54\u001b[0m     weighted_mse \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(y[left_indices]) \u001b[38;5;241m*\u001b[39m left_mse \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(y[right_indices]) \u001b[38;5;241m*\u001b[39m right_mse) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/numpy/core/_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"===============Self-Built XGBoost Regressor Baseline Outputs============\")\n",
    "xgb_predicted_values,xgb_model = XGBoostRegressorScratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placing All Models in Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions from X_test that are combined to be passed into the ensemble\n",
    "y_pred_stacked_predictions = np.column_stack((lin_reg_predicted_values, rfr_predicted_values,xgb_predicted_values,dtr_predicted_values))\n",
    "y_pred_stacked_predictions_df = pd.DataFrame(y_pred_stacked_predictions, columns=['reg_pred1', 'rf_pred2','xgboost_pred3','dtr_pred4'])\n",
    "\n",
    "#Generating Y_validation values from X_values\n",
    "y_val_input_reg_model = lin_reg_model.predict(x_val.values)\n",
    "y_val_input_rf_model=rfr_model.predict(x_val.values)\n",
    "y_val_input_xgb_model=xgb_model.predict(x_val.values)\n",
    "y_val_input_dtr_model=dtr_model.predict(x_val.values)\n",
    "\n",
    "y_val_input_model_predictions = np.column_stack((y_val_input_reg_model, y_val_input_rf_model,y_val_input_xgb_model,y_val_input_dtr_model))\n",
    "y_val_input_model_predictions_df = pd.DataFrame(y_val_input_model_predictions, columns=['reg_pred1', 'rf_pred2','xgboost_pred3','dtr_pred4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built Ensemble Model============================\n",
      "Validation RMSE: 2.4207257129378785\n",
      "[0.0302768  0.84246892 0.08208994 0.056302  ]\n",
      "Validation R2: 0.504791776720299\n",
      "Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Linear Reg Model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_model= run_linear_regression(y_pred_stacked_predictions_df,y_test) # Predicted Values of the Models, and y_test is the Y variable\n",
    "y_ensemble = ensemble_model.predict(y_val_input_model_predictions_df) \n",
    "\n",
    "val_rmse = root_mean_squared_error(y_ensemble, y_val)\n",
    "r2 = r2_score(y_ensemble, y_val)\n",
    "\n",
    "print(\"===============Self-Built Ensemble Model============================\")\n",
    "print(f'Validation RMSE: {val_rmse}')\n",
    "print(ensemble_model.weights)\n",
    "print(f'Validation R2: {r2}')\n",
    "\n",
    "print(\"Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Linear Reg Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built Ensemble Model (Ridge)=====================\n",
      "Validation RMSE: 2.420722136270257\n",
      "Validation R2: 0.6650582168762581\n",
      "Ensemble Model Weights (Coefficients): [0.03030364 0.84160629 0.08231033 0.05691816]\n",
      "Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ridge regression model for the ensemble\n",
    "ridge_model = run_ridge_regression(y_pred_stacked_predictions_df, y_test)\n",
    "\n",
    "# Make predictions with the trained Ridge model on the validation set\n",
    "y_ensemble = ridge_model.predict(y_val_input_model_predictions_df)\n",
    "\n",
    "# Evaluate the model\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_ensemble))\n",
    "r2 = r2_score(y_val, y_ensemble)\n",
    "\n",
    "# Print results\n",
    "print(\"===============Self-Built Ensemble Model (Ridge)=====================\")\n",
    "print(f'Validation RMSE: {val_rmse}')\n",
    "print(f'Validation R2: {r2}')\n",
    "\n",
    "print(f'Ensemble Model Weights (Coefficients): {ridge_model.weights}')\n",
    "\n",
    "print(\"Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built Ensemble Model (Ridge)=====================\n",
      "Best number of splits (n_splits): 20\n",
      "Best alpha: 10\n",
      "Validation RMSE: 2.420691730060248\n",
      "Validation R2: 0.6650666310978528\n",
      "Ensemble Model Weights (Coefficients): [[0.03054514 0.83397563 0.08426699 0.06235722]]\n",
      "Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model w Hyper Parameter Tuning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define parameter grid for alpha\n",
    "parameters = {'alpha': [0.01, 0.1, 1, 1.5, 1.6, 3, 3.01, 3.05, 3.1, 3.5, 5, 10, 100, 100]}\n",
    "\n",
    "# List of n_splits to try\n",
    "n_splits_options = [3, 5, 7, 10, 20, 30]\n",
    "best_score = float(\"inf\")\n",
    "best_n_splits = None\n",
    "best_alpha = None\n",
    "best_model = None\n",
    "\n",
    "# Loop through different values of n_splits\n",
    "for n_splits in n_splits_options:\n",
    "    # Set up KFold with the current number of splits\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Set up GridSearchCV with the current KFold\n",
    "    ridge_cv = GridSearchCV(Ridge(), parameters, scoring='neg_mean_squared_error', cv=kf)\n",
    "    ridge_cv.fit(y_pred_stacked_predictions_df, y_test)\n",
    "    \n",
    "    # Get the best alpha and score from the GridSearchCV\n",
    "    if -ridge_cv.best_score_ < best_score:\n",
    "        best_score = -ridge_cv.best_score_\n",
    "        best_n_splits = n_splits\n",
    "        best_alpha = ridge_cv.best_params_['alpha']\n",
    "        best_model = ridge_cv.best_estimator_\n",
    "\n",
    "# Fit the Ridge model with the best alpha and best n_splits\n",
    "ridge_model = Ridge(alpha=best_alpha)\n",
    "ridge_model.fit(y_pred_stacked_predictions_df, y_test)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_ensemble = ridge_model.predict(y_val_input_model_predictions_df)\n",
    "\n",
    "# Evaluate the model\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_ensemble))\n",
    "r2 = r2_score(y_val, y_ensemble)\n",
    "\n",
    "# Print results\n",
    "print(\"===============Self-Built Ensemble Model (Ridge)=====================\")\n",
    "print(f'Best number of splits (n_splits): {best_n_splits}')\n",
    "print(f'Best alpha: {best_alpha}')\n",
    "print(f'Validation RMSE: {val_rmse}')\n",
    "print(f'Validation R2: {r2}')\n",
    "print(f'Ensemble Model Weights (Coefficients): {ridge_model.coef_}')\n",
    "\n",
    "\n",
    "print(\"Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model w Hyper Parameter Tuning\")\n",
    "#Conclusion : Not very effective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built Ensemble Model (Ridge)=====================\n",
      "Validation RMSE: 2.420764220300993\n",
      "Validation R2: 0.6650465709111051\n",
      "Ensemble Model Weights (Coefficients): [0.03005253 0.84700613 0.08242635 0.05163351]\n",
      "Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize Lasso regression model for the ensemble\n",
    "ridge_model = run_lasso_regression(y_pred_stacked_predictions_df, y_test)\n",
    "\n",
    "# Make predictions with the trained Ridge model on the validation set\n",
    "y_ensemble = ridge_model.predict(y_val_input_model_predictions_df)\n",
    "\n",
    "# Evaluate the model\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_ensemble))\n",
    "r2 = r2_score(y_val, y_ensemble)\n",
    "\n",
    "# Print results\n",
    "print(\"===============Self-Built Ensemble Model (Ridge)=====================\")\n",
    "print(f'Validation RMSE: {val_rmse}')\n",
    "print(f'Validation R2: {r2}')\n",
    "\n",
    "print(f'Ensemble Model Weights (Coefficients): {ridge_model.weights}')\n",
    "\n",
    "print(\"Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Removing Linear Regresion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model predictions from X_test that are combined to be passed into the ensemble\n",
    "y_pred_stacked_predictions_nolrg = np.column_stack((rfr_predicted_values,xgb_predicted_values,dtr_predicted_values))\n",
    "y_pred_stacked_predictions_df_nolrg = pd.DataFrame(y_pred_stacked_predictions_nolrg, columns=['rf_pred2','xgboost_pred3','dtr_pred4'])\n",
    "\n",
    "#Generating Y_validation values from X_values\n",
    "y_val_input_rf_model=rfr_model.predict(x_val.values)\n",
    "y_val_input_xgb_model=xgb_model.predict(x_val.values)\n",
    "y_val_input_dtr_model=dtr_model.predict(x_val.values)\n",
    "\n",
    "y_val_input_model_predictions_nolrg = np.column_stack((y_val_input_rf_model,y_val_input_xgb_model,y_val_input_dtr_model))\n",
    "y_val_input_model_predictions_df_nolrg = pd.DataFrame(y_val_input_model_predictions_nolrg, columns=['rf_pred2','xgboost_pred3','dtr_pred4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Self-Built Ensemble Model (Ridge)=====================\n",
      "Validation RMSE: 2.4216553057854653\n",
      "Validation R2: 0.6647999322091093\n",
      "Ensemble Model Weights (Coefficients): [0.85201269 0.08306338 0.072124  ]\n",
      "Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ridge regression model for the ensemble\n",
    "final_ridge_ens_model = run_ridge_regression(y_pred_stacked_predictions_df_nolrg, y_test)\n",
    "\n",
    "# Make predictions with the trained Ridge model on the validation set\n",
    "y_ensemble = final_ridge_ens_model.predict(y_val_input_model_predictions_df_nolrg)\n",
    "\n",
    "# Evaluate the model\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_ensemble))\n",
    "r2 = r2_score(y_val, y_ensemble)\n",
    "\n",
    "# Print results\n",
    "print(\"===============Self-Built Ensemble Model (Ridge)=====================\")\n",
    "print(f'Validation RMSE: {val_rmse}')\n",
    "print(f'Validation R2: {r2}')\n",
    "\n",
    "print(f'Ensemble Model Weights (Coefficients): {final_ridge_ens_model.weights}')\n",
    "\n",
    "print(\"Ensemble Model - Linear Reg, RF, XGBoost, Decision tree Models stacked on Ridge Reg Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       RF_Predictions  XGBoost_Predictions  DecisionTree_Predictions  \\\n",
      "0           -0.915073            -0.121598                 -2.380586   \n",
      "1           -0.056161            -0.045450                  0.011772   \n",
      "2           -0.016856            -0.073922                  0.011772   \n",
      "3            0.066778             0.023744                  0.011772   \n",
      "4           -0.056161             0.049648                  0.011772   \n",
      "...               ...                  ...                       ...   \n",
      "38955       -2.378902            -2.594488                 -2.380586   \n",
      "38956        4.957676             5.024979                  4.970047   \n",
      "38957       -2.378902            -2.540333                 -2.517729   \n",
      "38958       -2.477288            -3.048090                 -2.781133   \n",
      "38959       -2.557912            -1.164137                 -1.819564   \n",
      "\n",
      "       y_ensemble  \n",
      "0       -0.954579  \n",
      "1       -0.050413  \n",
      "2       -0.019513  \n",
      "3        0.059290  \n",
      "4       -0.042571  \n",
      "...           ...  \n",
      "38955   -2.396801  \n",
      "38956    4.964113  \n",
      "38957   -2.402156  \n",
      "38958   -2.546120  \n",
      "38959   -2.390096  \n",
      "\n",
      "[38960 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate predictions on the entire DataFrame using the models\n",
    "merged_df['RF_Predictions'] = rfr_model.predict(X.values)  \n",
    "merged_df['XGBoost_Predictions'] = xgb_model.predict(X.values) \n",
    "merged_df['DecisionTree_Predictions'] = dtr_model.predict(X.values) \n",
    "\n",
    "# Coefficients as weights (from your ensemble model)\n",
    "coef_model1 = final_ridge_ens_model.weights[0] # Coefficient for Model 1 RF\n",
    "coef_model2 = final_ridge_ens_model.weights[1]# Coefficient for Model 2 XGB\n",
    "coef_model3 = final_ridge_ens_model.weights[2] # Coefficient for Model 3 Decision Tree\n",
    "\n",
    "# Normalize coefficients to sum to 1 (optional but common practice)\n",
    "total_coef = coef_model1 + coef_model2 + coef_model3\n",
    "weight_model1 = coef_model1 / total_coef\n",
    "weight_model2 = coef_model2 / total_coef\n",
    "weight_model3 = coef_model3 / total_coef\n",
    "\n",
    "\n",
    "# Calculate the weighted average predictions (y_ensemble)\n",
    "merged_df['y_ensemble'] = (\n",
    "    merged_df['RF_Predictions'] * weight_model1 +\n",
    "    merged_df['XGBoost_Predictions'] * weight_model2 + \n",
    "    merged_df['DecisionTree_Predictions'] * weight_model3 \n",
    ")\n",
    "\n",
    "# Display the updated DataFrame with the ensemble predictions\n",
    "print(merged_df[['RF_Predictions', 'XGBoost_Predictions','DecisionTree_Predictions', 'y_ensemble']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('ETH_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
