{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python notebook performs sentiment analysis using LLM Models from LLM Models.ipynb to generate sentiments at a multi-threaded level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "import csv\n",
    "import numpy as np\n",
    "import glob\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for BERT-based sentiment classification\n",
    "def sentimentWithBert(input_text):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "    inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    # index_of_largest = torch.argmax(predictions).item()\n",
    "    # sentiments = ['positive', 'negative', 'neutral']\n",
    "    \n",
    "    # sentiment = sentiments[index_of_largest]\n",
    "    \n",
    "    return predictions.detach().numpy()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'sampled_file_20', 'us_2024_news_sampled/sampled_file_20.csv')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(\"us_2024_news_sampled/*.csv\")\n",
    "fileInfoObjectList = []\n",
    "idx = 0\n",
    "for file in files:\n",
    "    fileName = file.split(\"/\")[-1]\n",
    "    fileName = fileName.split(\".\")[0]\n",
    "    fileInfoObjectList.append((idx,fileName,file))\n",
    "    idx+=1\n",
    "\n",
    "\n",
    "fileInfoObjectList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentiment(fileInfo):\n",
    "    idx = fileInfo[0]\n",
    "    fileName = fileInfo[1]\n",
    "    fileDir = fileInfo[2]\n",
    "\n",
    "    try:\n",
    "        raw_df = pd.read_csv(fileDir,header=None,on_bad_lines='skip',low_memory=False)\n",
    "        df = raw_df.copy()\n",
    "        df.columns = [\"DateTime\",\"URL\",\"Title\",\"SharingImage\",\"LangCode\",\"DocTone\",\"DomainCountryCode\",\"Location\",\"Lat\",\"Lon\",\"CountryCode\",\"Adm1Code\",\"Adm2Code\",\"GeoType\",\"ContextualText\",\"the_geom\",\"date\"]\n",
    "        df[\"prediction\"] = df.apply(lambda row : sentimentWithBert(row[\"ContextualText\"]),axis=1)\n",
    "        df[\"positive_fb\"] = df.apply(lambda row : row[\"prediction\"][0],axis=1)\n",
    "        df[\"negative_fb\"] = df.apply(lambda row : row[\"prediction\"][1],axis=1)\n",
    "        df[\"neutral_fb\"] = df.apply(lambda row : row[\"prediction\"][2],axis=1)\n",
    "        df.to_csv(f\"sentiment_sampled_processed/{fileName}_processed.csv\",index=False)\n",
    "        failure_msg = \"Succeeded\"\n",
    "        print(f\"{idx} {fileName} Succeeded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failure_msg = f\"{idx} file: {fileDir} failed with error: {e}\"\n",
    "        print(failure_msg)\n",
    "    \n",
    "    return (idx,fileDir,failure_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 sampled_file_81 Succeeded\n",
      "15 sampled_file_8 Succeeded\n",
      "18 sampled_file_30 Succeeded\n",
      "21 sampled_file_43 Succeeded\n",
      "12 sampled_file_19 Succeeded\n",
      "0 sampled_file_34 Succeeded\n",
      "27 sampled_file_56 Succeeded\n",
      "9 sampled_file_32 Succeeded\n",
      "6 sampled_file_36 Succeeded\n",
      "3 sampled_file_35 Succeeded\n",
      "25 sampled_file_95 Succeeded\n",
      "16 sampled_file_9 Succeeded\n",
      "28 sampled_file_40 Succeeded\n",
      "19 sampled_file_18 Succeeded\n",
      "22 sampled_file_94 Succeeded\n",
      "13 sampled_file_31 Succeeded\n",
      "4 sampled_file_23 Succeeded\n",
      "7 sampled_file_22 Succeeded\n",
      "1 sampled_file_20 Succeeded\n",
      "10 sampled_file_33 Succeeded\n",
      "26 sampled_file_42 Succeeded\n",
      "29 sampled_file_54 Succeeded\n",
      "23 sampled_file_80 Succeeded\n",
      "17 sampled_file_24 Succeeded\n",
      "20 sampled_file_57 Succeeded\n",
      "14 sampled_file_25 Succeeded\n",
      "5 sampled_file_37 Succeeded\n",
      "2 sampled_file_21 Succeeded\n",
      "11 sampled_file_27 Succeeded\n",
      "8 sampled_file_26 Succeeded\n",
      "39 file: us_2024_news_sampled/sampled_file_45.csv failed with error: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "30 sampled_file_68 Succeeded\n",
      "33 sampled_file_96 Succeeded\n",
      "36 sampled_file_55 Succeeded\n",
      "42 sampled_file_92 Succeeded\n",
      "45 sampled_file_50 Succeeded\n",
      "51 sampled_file_85 Succeeded\n",
      "48 sampled_file_52 Succeeded\n",
      "57 sampled_file_62 Succeeded\n",
      "54 sampled_file_47 Succeeded\n",
      "40 sampled_file_51 Succeeded\n",
      "31 sampled_file_83 Succeeded\n",
      "34 sampled_file_82 Succeeded\n",
      "37 sampled_file_41 Succeeded\n",
      "43 sampled_file_93 Succeeded\n",
      "46 sampled_file_44 Succeeded\n",
      "52 sampled_file_84 Succeeded\n",
      "49 sampled_file_46 Succeeded\n",
      "58 sampled_file_89 Succeeded\n",
      "55 sampled_file_53 Succeeded\n",
      "41 sampled_file_86 Succeeded\n",
      "32 sampled_file_97 Succeeded\n",
      "35 sampled_file_69 Succeeded\n",
      "44 sampled_file_87 Succeeded\n",
      "38 sampled_file_79 Succeeded\n",
      "47 sampled_file_78 Succeeded\n",
      "50 sampled_file_91 Succeeded\n",
      "53 sampled_file_90 Succeeded\n",
      "59 sampled_file_88 Succeeded\n",
      "56 sampled_file_76 Succeeded\n",
      "60 sampled_file_63 Succeeded\n",
      "63 sampled_file_75 Succeeded\n",
      "66 sampled_file_48 Succeeded\n",
      "69 sampled_file_58 Succeeded\n",
      "72 sampled_file_71 Succeeded\n",
      "75 sampled_file_73 Succeeded\n",
      "78 sampled_file_99 Succeeded\n",
      "81 sampled_file_15 Succeeded\n",
      "84 sampled_file_5 Succeeded\n",
      "87 sampled_file_16 Succeeded\n",
      "61 sampled_file_77 Succeeded\n",
      "67 sampled_file_74 Succeeded\n",
      "64 sampled_file_49 Succeeded\n",
      "70 sampled_file_64 Succeeded\n",
      "73 sampled_file_65 Succeeded\n",
      "76 sampled_file_67 Succeeded\n",
      "79 sampled_file_66 Succeeded\n",
      "82 sampled_file_29 Succeeded\n",
      "85 sampled_file_28 Succeeded\n",
      "88 sampled_file_7 Succeeded\n",
      "62 sampled_file_61 Succeeded\n",
      "68 sampled_file_60 Succeeded\n",
      "71 sampled_file_70 Succeeded\n",
      "74 sampled_file_59 Succeeded\n",
      "65 sampled_file_100 Succeeded\n",
      "77 sampled_file_98 Succeeded\n",
      "80 sampled_file_72 Succeeded\n",
      "83 sampled_file_4 Succeeded\n",
      "86 sampled_file_14 Succeeded\n",
      "89 sampled_file_6 Succeeded\n",
      "90 sampled_file_17 Succeeded\n",
      "93 sampled_file_3 Succeeded\n",
      "96 sampled_file_10 Succeeded\n",
      "99 sampled_file_39 Succeeded\n",
      "91 sampled_file_13 Succeeded\n",
      "94 sampled_file_12 Succeeded\n",
      "97 sampled_file_1 Succeeded\n",
      "92 sampled_file_2 Succeeded\n",
      "95 sampled_file_38 Succeeded\n",
      "98 sampled_file_11 Succeeded\n"
     ]
    }
   ],
   "source": [
    "with ThreadPool(10) as pool:\n",
    "    results = pool.map(generateSentiment, fileInfoObjectList)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    results_df = pd.DataFrame(data = results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 'sampled_file_45', 'us_2024_news_sampled/sampled_file_45.csv')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileInfoObjectList[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 file: us_2024_news_sampled/sampled_file_45.csv failed with error: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39,\n",
       " 'us_2024_news_sampled/sampled_file_45.csv',\n",
       " '39 file: us_2024_news_sampled/sampled_file_45.csv failed with error: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateSentiment(fileInfoObjectList[39])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
